# -*- coding: utf-8 -*-
"""modules.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1shQQOeGXQ5u7TMJ_0JqiSRgAkric-yM_
"""

import torch
from collections import OrderedDict

# neural networks for RNATPH
class NN(torch.nn.Module):
    def __init__(self, layers):
        super(NN, self).__init__()
        self.depth = len(layers) - 1
        self.activation = torch.nn.Tanh
        layer_list = list()
        for i in range(self.depth - 1):
            layer_list.append(('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])))
            layer_list.append(('activation_%d' % i, self.activation()))
        layer_list.append(('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])))
        layerDict = OrderedDict(layer_list)
        def init_weights(m):
          if isinstance(m, torch.nn.Linear):
            torch.nn.init.xavier_normal_(m.weight)
            if m.bias is not None: torch.nn.init.normal_(m.bias)
        self.layers = torch.nn.Sequential(layerDict)
        self.layers.apply(init_weights)

    def forward(self, t):
        return self.layers(t)

class SelfAdaptiveWeight(torch.nn.Module):
    class GradReverse(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x): return x.view_as(x)
        @staticmethod
        def backward(ctx, grad_output): return grad_output.neg()
    @classmethod
    def grad_reverse(cls, x): return cls.GradReverse.apply(x)
    def __init__(self, init_value):
        super().__init__()
        self.init_value = init_value
        self.weight = torch.nn.Parameter(self.init_value * torch.ones(1))
    def forward(self):
        return torch.exp(self.grad_reverse(self.weight))